# 问  
考虑一个常见场景：程序想通过 TCP 连接发送 100k 字节的数据，但是在 write() 调用中，操作系统只接受了 80k 字节（受 TCP advertised window 的控制，细节见 TCPv1），你肯定不想在原地等待，因为不知道会等多久（取决于对方什么时候接受数据，然后滑动 TCP 窗口）。程序应该尽快交出控制权，返回 event loop。在这种情况下，剩余的 20k 字节数据怎么办？
对于应用程序而言，它只管生成数据，它不应该关心到底数据是一次性发送还是分成几次发送，这些应该由网络库来操心，程序只要调用 TcpConnection::send() 就行了，网络库会负责到底。网络库应该接管这剩余的 20k 字节数据，把它保存在该 TCP connection 的 output buffer 里，然后注册 POLLOUT 事件，一旦 socket 变得可写就立刻发送数据。当然，这第二次 write() 也不一定能完全写入 20k 字节，如果还有剩余，网络库应该继续关注 POLLOUT 事件；如果写完了 20k 字节，网络库应该停止关注 POLLOUT，以免造成 busy loop。（Muduo EventLoop 采用的是 epoll level trigger，这么做的具体原因我以后再说。）
如果程序又写入了 50k 字节，而这时候 output buffer 里还有待发送的 20k 数据，那么网络库不应该直接调用 write()，而应该把这 50k 数据 append 在那 20k 数据之后，等 socket 变得可写的时候再一并写入。
如果 output buffer 里还有待发送的数据，而程序又想关闭连接（对程序而言，调用 TcpConnection::send() 之后他就认为数据迟早会发出去），那么这时候网络库不能立刻关闭连接，而要等数据发送完毕，见我在《为什么 muduo 的 shutdown() 没有直接关闭 TCP 连接？》一文中的讲解。
综上，要让程序在 write 操作上不阻塞，网络库必须要给每个 tcp connection 配置 output buffer。
以上是陈硕对于buffer必要性的解释的一部分，其中那个例子我不懂的是，系统调用send()，它一次性接受了80k的数据，那它的缓冲区(内核中的)满了吗？如果满了，假设对面接受缓冲区(同样内核中的)也很小，并且我设置了非阻塞模式，那send()是直接返回了吗？没发完的数据怎么办？如果send()的发送缓冲区(内核中的)没满，进程是等待数据发完再返回吗？

# 答  
你引用的这段陈硕老师的话，是理解高性能网络编程的“分水岭”。你提出的三个疑问，正好切中了 **Linux 内核协议栈**与**用户态网络库**交互的核心。

我们直接进入“显微镜”模式，逐一拆解：

---

### 1. 接受了 80k，内核缓冲区满了吗？

**答案是：是的，满了（或者说达到了此时允许的上限）。**

当 `write()` 或 `send()` 返回一个小于你请求发送字节数的值（比如 80k < 100k）时，在内核视角下发生了这件事：

* 内核尝试把数据从你的 **用户态内存** 拷贝到 **内核发送缓冲区（SO_SNDBUF）**。
* 拷贝到 80k 的时候，内核缓冲区填满了，或者触及了由 **TCP 滑动窗口** 限制的发送上限。
* 内核不再继续拷贝，而是立刻停下来，告诉进程：“我尽力了，这次只能吃下 80k。”

### 2. 非阻塞模式下，没发完的 20k 怎么办？

这是网络库（muduo）存在的最大意义之一。

* **`send()` 会立刻返回吗？**：是的。在非阻塞模式下，`send` 绝不等待。它把能拷走的 80k 拷走，然后返回 `81920`（80k）。它不会管剩下的 20k。
* **没发完的数据去哪了？**：
* 如果你没有网络库，这 20k 还在你的业务代码定义的变量里。
* 在 muduo 中，`TcpConnection::send()` 会发现 `send` 只发了 80k，于是它把剩下的 20k 默默地 **Append（追加）** 到该连接的 **Output Buffer（用户态输出缓冲区）** 中。
* 同时，muduo 会向 `Poller` 注册 **POLLOUT** 事件。这意味着：“内核啊，等你的发送缓冲区有空位了，记得通过 `epoll_wait` 叫醒我，我好把剩下的 20k 发出去。”



---

### 3. 如果发送缓冲区没满，`send()` 会等数据发完再返回吗？

**绝对不会。**

这是一个非常常见的误区。你需要记住：**`send()` 的返回并不代表数据已经发送到了网络，更不代表对方已经收到。**

* **`send()` 的本质**：它其实应该叫 `copy_to_kernel_buffer()`。
* **返回时机**：只要数据成功从你的 `std::string` 或 `char*` 拷贝到了 **内核发送缓冲区**，`send()` 就会立刻高高兴兴地返回。
* **后续动作**：至于数据什么时候变成网卡上的电信号或光信号，那是内核协议栈的事情，受 TCP 拥塞控制、滑动窗口、丢包重传等机制控制，与你的进程无关了。

---

### ⚖️ 总结对比：内核缓冲区 vs. 进程状态

| 场景 | 进程是否阻塞 | 谁负责剩下的数据 |
| --- | --- | --- |
| **阻塞 IO** | **是**。进程会停在 `send` 这一行，直到 100k 全部挤进内核。 | 内核（因为全部接管了）。 |
| **非阻塞 IO (不满)** | **否**。立刻返回 100k。 | 内核（已经全部拷贝进内核）。 |
| **非阻塞 IO (满/半满)** | **否**。立刻返回已拷贝的长度（如 80k）。 | **网络库 (muduo)**。负责把剩下的 20k 存起来并等待下次发送。 |

### 💡 为什么必须有 Output Buffer？

想象一下，如果没有 Output Buffer，当 `send` 只发了 80k 时，你的业务逻辑必须自己写一个循环去盯着这个 Socket，或者停下手头所有活计去等。这会让 Reactor 模式“一线程多连接”的优势荡然无存。

有了 Output Buffer，业务层只需要调用一次 `TcpConnection::send(huge_data)`，然后就可以优雅地去处理其他客户的请求了。剩下的“断断续续”的发送任务，由 muduo 的 `handleWrite()` 回调在后台自动完成。

---

**你现在理解了为什么 `TcpConnection` 需要 `handleWrite` 这么一个回调了吗？要不要我带你看看在 muduo 源码里，这个 `handleWrite` 是如何被触发并最终把那残余的 20k 彻底发掉的？**

# 问  
那如果对端接受缓冲区很有问题，send()的内核缓冲区一直是满的会怎么样？  

# 答  
这是一个非常经典的网络编程“死锁”场景。如果对端的**接收能力**出了问题（比如由于业务逻辑卡死，完全不调用 `recv`），或者网络链路极差，会引发一系列连锁反应。

在 muduo 这种基于 Reactor 的非阻塞网络库中，情况会演变成如下过程：

### 1. 第一阶段：内核缓冲区塞满 (Kernel Side)

由于 TCP 的**流量控制（Flow Control）**机制，对端接收缓冲区满了会向你发送“零窗口（Zero Window）”通知。

* 此时，你的**内核发送缓冲区**会迅速堆满。
* 此时任何 `write/send` 调用都会返回 `EAGAIN` 或 `EWOULDBLOCK`。

### 2. 第二阶段：应用层缓冲区爆炸 (User Side)

由于 muduo 承诺了“只要你调用 `send()`，我就负责发完”，它会不断地将你发出的新数据 **Append（追加）** 到该连接的 `outputBuffer_` 中。

* **内存隐患**：如果你的程序还在源源不断地产生数据并调用 `send()`，而内核缓冲区又迟迟腾不出空间，`outputBuffer_` 就会像吹气球一样膨胀。
* **后果**：如果没有任何限制，最严重的后果是 **OOM (Out of Memory)**，即应用层缓冲区耗尽了系统所有内存，导致进程被操作系统强行 Kill 掉。

---

### 3. muduo 的应对之道：高水位回调 (High Water Mark)

陈硕老师在设计 muduo 时预见到了这种危险，因此提供了一个保护机制：**HighWaterMark Callback**。

你可以给 `TcpConnection` 设置一个“高水位线”（比如 64MB）：

```cpp
connection->setHighWaterMarkCallback(onHighWaterMark, 64*1024*1024);

```

当 `outputBuffer_` 的积压量超过这个界限时，muduo 会触发 `onHighWaterMark` 回调。在这个回调里，你通常有几种选择：

1. **停止发送**：暂停产生新数据的逻辑。
2. **断开连接**：如果这是一个恶意卡住的客户端，直接踢掉它。
3. **日志报警**：记录这个异常的连接。

---

### 4. 极端情况：进程“饿死”了吗？

回到你最初担心的“饿死”问题：

* **线程层面**：**不会饿死**。因为 `send` 是非阻塞的，`EventLoop` 线程依然在飞速转动，处理其他正常的连接。
* **业务层面**：**会影响性能**。虽然线程没死，但它会频繁被这个“满员”的 Socket 唤醒。
* 每次 `epoll_wait` 返回，发现这个 Socket 依然“可写”，于是触发 `handleWrite`。
* `handleWrite` 尝试调用 `write`，结果内核返回 `EAGAIN`。
* 这一轮循环就白费了。



> **这就是为什么 muduo 在写完数据后必须立刻注销 POLLOUT 事件。** 只有当 `outputBuffer_` 里真的有东西要发，且内核缓冲区有空位时，才去关注可写事件。

---

### ⚖️ 总结：对端不收，我会怎样？

| 层面 | 表现 | 最终后果 |
| --- | --- | --- |
| **内核态** | 发送缓冲区持续处于 `Full` 状态 | 停止向网卡发包，进入零窗口探测阶段 |
| **用户态** | `outputBuffer_` 持续增长 | 若无干预，最终导致 OOM (内存溢出) |
| **Reactor 线程** | 依然活跃，处理其他连接 | 会因频繁处理该连接的无效 `handleWrite` 造成少量 CPU 损耗 |

**这就是为什么“应用层 Buffer”虽然必要，但必须配合“流量控制逻辑”的原因。你想看看在 muduo 代码中，具体是在哪里判断“数据写完了”并注销 `POLLOUT` 事件的吗？**

# 问  
那是不同的对端在内核中对应的发送缓冲区也不同吗？也就是说不同的连接，内核给它分配不同的缓冲区发送，缓冲区是和Socket有关的吗？  

# 答  
这是一个非常核心的问题，答案是肯定的：**每个 Socket 连接在内核中都拥有独立且私有的发送和接收缓冲区。**

你可以把 Linux 内核想象成一个大型物流中转站，而每一个 TCP 连接就是一条专属的流水线。

---

### 1. 缓冲区与 Socket 的绑定关系

在 Linux 内核中，每个 Socket 都由一个 `struct sock` 结构体表示。在这个结构体内部，维护着两个关键的队列（即缓冲区）：

* **`sk_write_queue`**：内核发送缓冲区。
* **`sk_receive_queue`**：内核接收缓冲区。

这意味着：

* **独立性**：连接 A 的缓冲区满了，完全不会影响连接 B。连接 B 只要内核缓冲区有空间，你的 `send()` 调用依然可以秒回。
* **资源消耗**：如果你开了 10,000 个并发连接，内核就会在内存里维护 10,000 组缓冲区。这也是为什么高并发服务器非常吃内存的原因。

---

### 2. 缓冲区的大小是谁决定的？

虽然每个 Socket 都有自己的缓冲区，但它们的大小并不是无限的，也不是完全固定的：

1. **系统默认值**：内核有全局的默认配置（可以通过 `cat /proc/sys/net/ipv4/tcp_wmem` 查看）。
2. **动态调整（Auto-tuning）**：现代 Linux 内核会根据网络状况（带宽延时积）自动调整每个 Socket 的缓冲区大小。
3. **人工干预**：你可以通过 `setsockopt` 手动设置某个特定 Socket 的缓冲区大小：
```cpp
int snd_size = 64 * 1024; // 64k
setsockopt(sockfd, SOL_SOCKET, SO_SNDBUF, &snd_size, sizeof(snd_size));

```



---

### 3. 回到 muduo 的视角：两级缓冲架构

现在我们可以把整个画面拼凑完整了。在 muduo 构建的多线程网络模型中，数据流是这样的：

* **用户态 (User Space)**：
* `TcpConnection A` -> 拥有自己的 `muduo::Buffer outputBuffer_`
* `TcpConnection B` -> 拥有自己的 `muduo::Buffer outputBuffer_`


* **内核态 (Kernel Space)**：
* `Socket A` -> 对应内核中的 `Send Buffer A`
* `Socket B` -> 对应内核中的 `Send Buffer B`



#### 场景模拟：

如果**对端 A** 故意不收数据，导致 **内核 Send Buffer A** 满了：

1. 你对 `TcpConnection A` 调用 `send()`。
2. muduo 发现 **内核 Send Buffer A** 满了，把数据存入 **用户态 outputBuffer_ A**。
3. 此时，你对 `TcpConnection B` 调用 `send()`。
4. 因为 **内核 Send Buffer B** 是独立的且是空的，数据直接拷贝进内核，`send()` 瞬间成功返回。

**结论：** 内核缓冲区的这种“按连接分配”的特性，是 Reactor 模式能够实现“不同连接之间互不干扰”的物理基础。

---

### 💡 进阶思考：既然内核缓冲区是独立的，那 muduo 线程会累死吗？

虽然连接 A 堵塞不会影响连接 B 的数据发送，但如果连接 A 的 `outputBuffer_` 积压了大量数据，每当 **内核 Send Buffer A** 腾出 1 字节的空间，`epoll` 就会触发一次 `POLLOUT` 事件，唤醒 `EventLoop` 线程去尝试写数据。

如果这种“慢连接”非常多，线程会频繁被唤醒做“无用功”。

**你想了解 muduo 是如何通过“只在需要时才注册 POLLOUT”来巧妙规避这种 CPU 浪费（Busy Loop）的吗？**